<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Epsilon Heap</title><link>https://epsilonheap.github.io/</link><description>This is a demo site for Nikola.</description><atom:link href="https://epsilonheap.github.io/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2019 &lt;a href="mailto:joe@demo.site"&gt;Your Name&lt;/a&gt; </copyright><lastBuildDate>Sun, 10 Feb 2019 03:56:12 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Kelly with Drawdown</title><link>https://epsilonheap.github.io/posts/kelly-with-drawdown/</link><dc:creator>Your Name</dc:creator><description>&lt;div&gt;&lt;h2&gt;Kelly Gambling&lt;/h2&gt;
&lt;p&gt;Portfolio management is a pillar of asset management. Given resources and many avenues for allocating said resources, how to allocate so as to achieve a desired effect? With regards to money, more is considered best, and with little chance of losing much would be ideal. Two views have received considerable attention on this topic. One approach is known as '&lt;a href="https://www.investopedia.com/terms/m/modernportfoliotheory.asp"&gt;Modern Portfolio Theory&lt;/a&gt;' and the other is '&lt;a href="https://www.investopedia.com/articles/trading/04/091504.asp"&gt;Kelly Criterion&lt;/a&gt;'. Warren Buffet &lt;a href="http://undergroundvalue.blogspot.com/2008/02/notes-from-buffett-meeting-2152008_23.html"&gt;explains&lt;/a&gt; the difference when asked by students: take advantage of an 'edge' where possible, and seek safety in numbers otherwise.&lt;/p&gt;
&lt;p&gt;Modern Portfolio Theory is taught in academia, and Kelly Criterion is mostly a method encountered by those asking 'is there another way?' This article on &lt;a href="https://ergodicityeconomics.com/2018/02/16/the-trouble-with-bernoulli-1738/#more-3206"&gt;Bernoulli&lt;/a&gt; and another on &lt;a href="https://ergodicityeconomics.com/2017/07/18/doing-a-laplace/#more-102"&gt;Laplace&lt;/a&gt; examine the fundamental difference of the two views and advocate for the Kelly approach. The latter article explains why the Kelly approach is less well known. Incidently, the criterion named after John Kelly is an of application of Claude Shannon's work to gambling - as detailed by &lt;a href="http://home.williampoundstone.net/Kelly.htm"&gt;William Poundstone&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Enzo Busseti, Ernest Ryu, and &lt;a href="http://web.stanford.edu/~boyd/"&gt;Stephen Boyd&lt;/a&gt; (who oversees several areas of exciting research) published '&lt;a href="https://arxiv.org/abs/1603.06183"&gt;Risk-Constrained Kelly Gambling&lt;/a&gt;' which derives an approach to incorporate &lt;a href="https://www.investopedia.com/terms/d/drawdown.asp"&gt;drawdown&lt;/a&gt; with Kelly gambling. When speaking of risks, drawdown would be top of the list for many - more so when following a Kelly approach. With concentrated positions, the prospects of some big bets going awry would be disastrous whereas a highly diversified portfolio would suffer the same fate as the market as a whole. This paper combines Kelly gambling with risk aversion in a systematic way, and do so by casting the problem as &lt;a href="http://web.stanford.edu/~boyd/cvxbook/"&gt;convex optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{cl}
maximize &amp;amp; \mathbb{E}\log({r^\intercal b}) \cr[2ex]
subject \; to &amp;amp; \begin{aligned}
\mathbb{E}(r^\intercal b)^{-\lambda} &amp;amp; \leq 1 \cr
\mathbb{I}^\intercal b &amp;amp; = 1, b \geq 0
\end{aligned}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;where \(r\) is a matrix of returns for a set of investments, \(b\) is a vector of allocations, and \(r^\intercal b\) is then the gain in wealth of a portfolio. \( \mathbb{E}(r^\intercal b)^{-\lambda} \leq 1 \) will be expanded upon later. This constraint effectively bounds how much risk one is willing to take.&lt;/p&gt;
&lt;p&gt;A key result of the paper is that \( \mathbb{E}(r^\intercal b)^{-\lambda} \leq 1 \Rightarrow \mathbb{Prob}(W^{min} \lt \alpha) \lt \beta\). That is, the probability \(W^{min}\), a random variable of the minimum wealth, of being less that \(\alpha\) is less than \(\beta\), and is equivalent to the aforementioned 'degree' of risk constraint. A different symbolic way to see the interplay of \(\alpha, \: \beta, \: and, \: \lambda \) is \( \mathbb{Prob}(W^{min} \lt \alpha) \lt \alpha^\lambda = \beta \) where \( \lambda = \frac{\log\beta}{\log \alpha} \).&lt;/p&gt;
&lt;p&gt;The paper carefully presents several variations of the Kelly gamble, establishes bounds, and perform a comparative study, but what is highlighted above constitutes the heart of the matter for realistic applications.&lt;/p&gt;&lt;/div&gt;</description><guid>https://epsilonheap.github.io/posts/kelly-with-drawdown/</guid><pubDate>Thu, 07 Feb 2019 20:43:54 GMT</pubDate></item><item><title>Time series</title><link>https://epsilonheap.github.io/posts/Time%20series/</link><dc:creator>Your Name</dc:creator><description>&lt;div&gt;&lt;h2&gt;Time Series&lt;/h2&gt;
&lt;p&gt;Educated as an Electrical engineer early in life and having more recently worked as a 'quant' for many years,the topic of time series analysis has evolved from a clean one to a bewildering hodgepodge of complications.&lt;/p&gt;
&lt;p&gt;Some fundamental issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Transactional - data from markets are records of transactional events and not uniformly sampled; therefore, times series are non-homogeneous.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stationarity - market data are not necessarily stationary and often have complicated time-delayed feedback structures. Moreover, change points (or regime shifts) are not uncommon.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ergodicity - practitioners often analyze time series to aid in prediction; but under what condition can the histories of (single realizations of)sampled paths be considered representative of future behavior?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are other issues like low signal-to-noise, high dimensionality, missing values, and outliers. Although these are important, the three highlighted undermine the applicability of traditional methods.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Transactional&lt;/h3&gt;
&lt;p&gt;Traditionally an engineer dealing with signals mainly work with Fourier transforms and related objects.&lt;/p&gt;
&lt;p&gt;$$
f(x) = \int_{-\infty}^{\infty} \hat f(\xi) e^{2 \pi i \xi x} d\xi 
$$&lt;/p&gt;
&lt;p&gt;Clean and elegant. This formalism is fundamental and will not likely ever diminish in stature. Yet quantitative-minded participants in finance, in particular currency traders of the late 1990's, pointed out a significant characteristics of the data of their trade that needed addressing:
&lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=208278"&gt;Operators on Inhomogeneous Time Series&lt;/a&gt;. A central idea underlying inhomogeneous time series is that 'time' is proportional to the density of 'activity'. With regards to the 'information' content of inhomogeneous market data, this paper &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1989555"&gt;'Discerning Information from Trade Data'&lt;/a&gt; describes a aggregated 'tick' or 'bar' approach - essentially integrating pieces of data partitioned by time into informational units for analysis. Aside: a good presentation of the many pitfalls encountered by financial professionals are in Marcos Lopez de Prado's &lt;a href="https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482109"&gt;'Advances in Financial Machine Learning'&lt;/a&gt;. A Jupyter notebook in the blog post &lt;a href="http://www.blackarbs.com/blog/exploring-alternative-price-bars"&gt;'Exploring Alternative Price Bars'&lt;/a&gt; illustrates this approach.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Non-Stationary Data&lt;/h3&gt;
&lt;p&gt;Over an intermediate time-span, market data fluctuate in a relatively well-behaved manner, and statistical observations like mean-reversion are relevant and expected. On a longer time-scale, statistically 'rare' events are not so uncommon in real markets. In other words, statistical models stop making sense when markets go wild - a good indicator that market prices are not necessarily stationary (see &lt;a href="https://www.youtube.com/watch?v=Pn_RiDbK82M&amp;amp;t=160s"&gt;Integration, Cointegration, and Stationarity&lt;/a&gt; for examples of non-stationary time series that are rather tame. Beware though that using integer differentiation may remove information content. See chapter 5 of &lt;a href="https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482109"&gt;'Advances in Financial Machine Learning'&lt;/a&gt; or look up fractional differentiation for financial data.)&lt;/p&gt;
&lt;!---
memory effects
--&gt;

&lt;p&gt;Bottom line is that market data needs to be analyzed and transformed before statistical methods are applicable as intended. Then again, many methods of Machine Learning work well in practice without a rigorous understanding of why they work. In any case, a usual suspect for why a model has limited or no predictive ability post construction is because the model was formulated with non-stationary data. Good sites examining fundamental issues in applying scientific theory to economics is &lt;a href="https://ergodicityeconomics.com/"&gt;Ergodicity Economics&lt;/a&gt; and to markets is &lt;a href="http://www.quantresearch.info/"&gt;Quantitative Finance&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Ergodicity&lt;/h3&gt;
&lt;p&gt;Equivalence of ensemble-average and time-average with respect to some sampled statistics is an important subject. &lt;a href="https://arxiv.org/abs/1401.7224"&gt;Gaveau and Schulman&lt;/a&gt; questioned whether ergodicity is a reasonable hypothesis - many applications only require 'reasonably'-sized samples. An article by &lt;a href="https://statweb.stanford.edu/~cgates/PERSI/papers/mixing.pdf"&gt;Persi Diaconis&lt;/a&gt; explore this for practitioners. &lt;a href="https://www.youtube.com/watch?v=LGqOH3sYmQA"&gt;Ole Peters&lt;/a&gt; has put forth commendable effort at explaining
the real-world implications of using ensemble quantities when individuals experience but one life. &lt;a href="https://www.youtube.com/watch?v=qA_6BWkC4og"&gt;Nassim Taleb&lt;/a&gt; discusses ergodicity from a trader's perspective.&lt;/p&gt;
&lt;p&gt;Even though the age of immense computing power and massive data collection is upon us, A better understanding of how many representative examples are enough for a good approximation is more important then ever. Reason being that society is putting more faith than ever into systems trained by examples, whether they are generated (games and such) or collected.&lt;/p&gt;
&lt;hr&gt;&lt;/div&gt;</description><guid>https://epsilonheap.github.io/posts/Time%20series/</guid><pubDate>Mon, 04 Feb 2019 19:34:54 GMT</pubDate></item><item><title>first tiny bit</title><link>https://epsilonheap.github.io/posts/first-tiny-bit/</link><dc:creator>Your Name</dc:creator><description>&lt;p&gt;Trying out different site generators. Nikola is first. Yea! \( e^{ix} = \cos{x} + i \sin{x} \) works fine!
Now try adding [GatsbyJS](&lt;a class="reference external" href="https://www.gatsbyjs.org/"&gt;https://www.gatsbyjs.org/&lt;/a&gt;) under the Nikola structure to take advantage of the dynamic features of that infrastructure.&lt;/p&gt;</description><guid>https://epsilonheap.github.io/posts/first-tiny-bit/</guid><pubDate>Sat, 02 Feb 2019 15:07:54 GMT</pubDate></item></channel></rss>