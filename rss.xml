<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Epsilon Heap</title><link>https://epsilonheap.github.io/</link><description>This is a demo site for Nikola.</description><atom:link href="https://epsilonheap.github.io/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:joe@demo.site"&gt;Your Name&lt;/a&gt; </copyright><lastBuildDate>Fri, 14 Jun 2019 11:17:04 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Application of Machine Learning in Quantitative Finance</title><link>https://epsilonheap.github.io/posts/application-of-machine-learning-in-quantitative-finance/</link><dc:creator>Your Name</dc:creator><description>&lt;div&gt;&lt;hr&gt;
&lt;h2&gt;Two Worlds Colliding&lt;/h2&gt;
&lt;p&gt;Markets are noisy and ever changing; moreover, fluctuations can be rather large and persistent. The nature and character of markets offer significant challenges for machine learning (ML) algorithms. For instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;low signal-to-noise&lt;ul&gt;
&lt;li&gt;'signals' that convey information about markets are often highly obscured by 'noise'.&lt;/li&gt;
&lt;li&gt;ML algorithms are designed to recognize or approximate patterns that are the most prevalent, and market are mostly noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;regime changes&lt;ul&gt;
&lt;li&gt;markets are constantly evolving, fluctuations can be large, and statistical characteristics can change quickly.&lt;/li&gt;
&lt;li&gt;current ML algorithms are specialists that do not handle outliers and new situations well.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;market inferences should be robust&lt;ul&gt;
&lt;li&gt;'signals' are not useful for market predictions if slight differences in what is observed lead to very different predictions.&lt;/li&gt;
&lt;li&gt;some ML methods are brittle - see &lt;a href="https://www.youtube.com/watch?v=Yr1mOzC93xs"&gt;video&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1711.11561"&gt;paper&lt;/a&gt; by Bengio - and small difference in input lead to notably different predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Recognition of the conflicting nature of markets and the current crop of ML algorithms is important in devising new approaches to using the ideas learned from ML in quantitative finance.&lt;/p&gt;
&lt;p&gt;Emphasis here is on introducing the powerful, foundational ideas of ML to the design of trading algorithms as oppose to using existing ML methods as tools for trading. Or as &lt;a href="https://www.youtube.com/watch?v=CW0DUg63lqU"&gt;Steve Jobs&lt;/a&gt; famously quoted of Picasso in an interview: “good artists borrow, great artists steal.”&lt;/p&gt;
&lt;p&gt;Caveat: The secretive world of traders mean that what is presented here could very well have been known for decades, and this work is embarrassingly elementary. &lt;/p&gt;
&lt;h2&gt;Ideas That Transcend&lt;/h2&gt;
&lt;p&gt;What ideas are considered great and transcend boundaries are to some degree subjective. Nevertheless, the following ideas from ML are at least worthy of consideration.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;1) Regularization of complexity as part of the learning process.
2) Boosting a set of weak learners into a single strong learner.
3) Stochastic gradient descent to drive exploration.
&lt;/pre&gt;


&lt;p&gt;There are more but these are enough to form the foundation of a trading algorithm with desirable properties. The missing pieces are a model and the data structure(s) to represent the model, a decision mechanism to establish what is 'better', and a set of features.&lt;/p&gt;
&lt;p&gt;The adage that, given a set of relevant features that span the distinguishing properties of the patterns of interest, just about any machine learning algorithm will perform well is not far from the truth. However, in the context of trading signals for markets though, these features will be very noisy, their relevance may be fleeting, yet they need to be robust. 'Quants' who have applied ML methods to trading will attest to significant impact of these problems. A trading algorithm would likely be well-served by addressing these inherent characteristics of trading signals.&lt;/p&gt;
&lt;p&gt;Domain expertise is important for trading. End-to-end techniques like Deep Neural Nets may someday 'crack' the code and derive effective representations that are also interpretable. Meanwhile, developing methods that incorporate existing understanding of market behavior is the goal. A major assumption is the availability of a set of 'weak' features in the sense described above.&lt;/p&gt;
&lt;h2&gt;Restate Again&lt;/h2&gt;
&lt;p&gt;Let's match what machine learning has to offer to the difficulties that need to be addressed. The term 'alpha' will be loosely associated with informative signals that are conducive for trading decisions.&lt;/p&gt;
&lt;p&gt;Regularization can be interpreted in several ways. A useful view is that regularization is used to distinguish 'noise' from 'signal' with regards to the weights parameterizing a family of models under consideration. 'Signal' weights should be much larger than 'noise' weights. Something like regularization is needed for trading features. In addition to reducing the 'noise' associated with an alpha, parsimonious models are more robust and generalizes better.&lt;/p&gt;
&lt;p&gt;Boosting, as in techniques like AdaBoost and Random Forest, takes a set of weak learners that can be implemented in a uniform manner, and specify a scheme that allows for each to contribute, as part of a larger collective, to the classification or approximation problem at hand. In addition, each weak learner is encouraged to be a specialist that focuses its predictive powers on a limited subset of the whole domain. An assumption is that successful quantitative trading is possible given a set of features with at least some alpha. If so, boosting seems to be an attractive methodology for trading.&lt;/p&gt;
&lt;p&gt;Stochastic gradient descent, by adding randomness and averaging over a subset to the directed exploration process in learning, enhances the robustness and predictive capabilities of ML algorithms. In addition to making learning large problems (high dimensions and large data sets) possible, sampling and averaging allows for excursions that may not occur if considered as a whole. Introduction of randomness mitigates the undesirable tendency to be 'stuck' in saddle points of high dimensional curved spaces during learning. Again, these are attractive properties that address what is problematic in trading.&lt;/p&gt;
&lt;h2&gt;Promising Start&lt;/h2&gt;
&lt;p&gt;Under the directive of incorporting fundamental ideas from ML to enhance existing practices in quantitaive trading, a simple algorithm was constructed with components analogous to those from ML described in the last section. Again, the emphasis in on capturing the esscense of the role rather than duplicating the algorithms in a differnt setting.&lt;/p&gt;
&lt;p&gt;Alas, the nature of trading is to share little and protect intellectual properties. Therefore, details will be scant. The main take-away is that this direction of research could be fruitful. In a business where most everyone is sharp and hungry, some hints may be all that is needed to give away everything. Here goes ...&lt;/p&gt;
&lt;p&gt;The work presented here were developed on the &lt;a href="https://www.quantopian.com/"&gt;Quantopian&lt;/a&gt; platform. Some well-studied signals with 'weak' alphas - they do not work anymore today - were examined for the possibility of being enhanced in systematic fashion.&lt;/p&gt;
&lt;p&gt;Given that the results are compared on a relative basis, practicle concerns such as commissions (fees paid to trade) and slippage (advertised prices are less that what is actually paid) could be neglected. But, these results do include slippage and commission. They were developed with the hopes of being profitable algorithms after all. The period will be from 01/04/2011 to 07/30/2012 where the signals under consideration still had some alphas, and a starting capital of $10,000,000. Slippage is 3 basis points with a volume limit of 0.1. Per trade cost is $2.95. Do not worry if these numbers are gibberish to you.&lt;/p&gt;
&lt;h3&gt;Signals (or Factors)&lt;/h3&gt;
&lt;p&gt;The signals used are commonly refered to as factors. Factors are quantities - eg. price/earnings (P/E) - associate with an equity which are then sorted by some criteria. The sorted list of values are then typically bucketed into quantiles. The virtue of being in a quantile is consider indicative of some important difference from objects in another quantile. For instance, the premise may be that equities within the top 10% of P/E will behave markedly different from equities in the botton 10% in a manner that could lead to profitable trades.&lt;/p&gt;
&lt;p&gt;For this demonstration, the factors under consideration are SimpleMovingAverage(...) of prices (and ratios of) as provided by the Quantopian libraries &lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;sma_10 = SimpleMovingAverage(inputs=
                            [USEquityPrices.close],
                            window_length=10,
                            mask=shorts_mask)
sma_3  = SimpleMovingAverage(inputs=
                            [USEquityPrices.close],
                            window_length=3,
                            mask=shorts_mask)
factor = (sma_3/sma_10)
&lt;/pre&gt;


&lt;p&gt;and a slight variation of &lt;a href="https://www.quantopian.com/posts/factor-analysis-momentum-rank"&gt;Momentum&lt;/a&gt; coded as &lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kr"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;Momentum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;CustomFactor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="s2"&gt;""" Momentum Factor """&lt;/span&gt;
&lt;span class="nx"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;USEquityPrices&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;close&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nx"&gt;Returns&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;window_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="nx"&gt;window_length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;252&lt;/span&gt;

&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;compute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;today&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;assets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;out&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;returns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;252&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;252&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nanstd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;returns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;These factors will be used for selection in a long/short (L/S) portfolio with a dollar weight of 54% longs and 46% shorts. Instead of quantiles, the top and botton 100 securities are candidates for investment allocation. On any day, the actual number of longs and shorts can vary depending on liquidity considerations.&lt;/p&gt;
&lt;p&gt;[TBD ...]&lt;/p&gt;&lt;/div&gt;</description><guid>https://epsilonheap.github.io/posts/application-of-machine-learning-in-quantitative-finance/</guid><pubDate>Wed, 12 Jun 2019 19:18:53 GMT</pubDate></item><item><title>Wellspring of Innovation</title><link>https://epsilonheap.github.io/posts/wellspring-of-innovation/</link><dc:creator>Your Name</dc:creator><description>&lt;div&gt;&lt;h2&gt;Manufacturing Innovation?&lt;/h2&gt;
&lt;p&gt;That rarest product of intellectual activities, original innovation, is often serendipitous. Many organizations are well aware of the elusive nature of innovation and yet persists in the pursuit of research and development (jokingly depicted as &lt;a href="https://www.google.com/search?tbm=isch&amp;amp;sa=1&amp;amp;ei=1ttmXOaMMuKGggfI9oyYCw&amp;amp;q=far+side+cat+herding+cartoons&amp;amp;oq=far+side+cat+herding+cartoons&amp;amp;gs_l=img.3...0.0..27395...0.0..0.0.0.......1......gws-wiz-img.HTpJM9I9_yk"&gt;herding cats&lt;/a&gt;) in hopes of finding 'the next big thing'. One approach that has demonstrated success is to cultivate a certain culture and attract individuals with specific characteristics to foster a 'breeding' ground for creative risk-taking. Noted omissions are the mention of 'experience', 'skill-set', or knowledge of specific technologies or academic subjects: a deliberate choice. The intended differentiation is to separate products of intellectual activities or processes from the factors that drive innovation. Again, the emphasis is not refinement nor incremental improvements but finding something 'new'. Find people that will find a way forward, even (or especially) if they don't know what they are doing.&lt;/p&gt;
&lt;h2&gt;"Taste"&lt;/h2&gt;
&lt;p&gt;Two examples of organizations that have engineered innovation on a large-scale are Bell Labs and Xerox. There are others but these two seem particularly influential. One common theme shared by the two is the rather nebulous word "taste".&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.johnseelybrown.com/"&gt;John Seely Brown&lt;/a&gt; (JSB) ran Xerox's research organization during my time there. &lt;a href="https://www.youtube.com/watch?v=amdONcqQvnU"&gt;Here&lt;/a&gt; is a fun and informative video of his take on research at Xerox. JSB uses a triad of aspiration, imagination, and intuition as pillars supporting his concept of 'taste'. The culture that he described was real and clearly evident in how research managers operated - &lt;a href="https://www.xerox.com/en-us/about/executive-leadership/corporate-officers/steve-hoover-biography"&gt;Steve Hoover&lt;/a&gt;, now CTO at Xerox, expounded these same core ideas when our discussions turned to how research was done at Xerox.
JSB's description of the preference for 'edge' workers and the importance of 'taste' is traditional and was passed down from the days of &lt;a href="https://www.wired.com/2017/04/youve-never-heard-tech-legend-bob-taylor-invented-almost-everything/"&gt;Robert Taylor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Yet another legendary organization was Bell Labs. &lt;a href="https://history.computer.org/pioneers/hamming.html"&gt;Richard Hamming&lt;/a&gt; was in the thick of things there and gave a personalized set of lectures (which starts &lt;a href="https://www.youtube.com/watch?v=AD4b-52jtos"&gt;here&lt;/a&gt;) on being successful in intellectual pursuits.  He also mentioned 'taste' as being vital to channeling efforts into good work.&lt;/p&gt;
&lt;h2&gt;From Elusive to Vague&lt;/h2&gt;
&lt;p&gt;So ... an elusive goal (innovation) could very well be the product of some vague notion (taste), amongst other things. Not helpful. Then let's see what other thinking are not helpful?&lt;/p&gt;
&lt;p&gt;An over reliance on pure technical abilities or specific knowledge as indicative of likely success. Science and technologies are evolving fast and paying heed to &lt;a href="https://quoteinvestigator.com/2014/05/04/adapt/"&gt;Darwin&lt;/a&gt; may be good advice: adaptability should be a prized trait. From a cognitive perspective, adaptability is a manifestation of curiosity, imagination, discipline, and astuteness. Advice: spend your resources on finding individuals with these characteristics, not those who do well on coding challenges or did well in school (remember the stated goal is innovation.) A slide in JSB's talk clearly said: 'Never looked at a transcript'.&lt;/p&gt;
&lt;p&gt;To wit: Tensorflow, in a short time-span, will have changed from one execution model (see &lt;a href="https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8"&gt;medium article&lt;/a&gt; and &lt;a href="https://www.reddit.com/r/MachineLearning/comments/9ysmtn/d_debate_on_tensorflow_20_api/"&gt;reddit discussion&lt;/a&gt;) to another, and users will have to essentially learn a new system. Yes, just one example, but this is not an isolated case. Technology may be fleeting, fundamentals have a much longer self-life.&lt;/p&gt;
&lt;p&gt;Another would be 'high energy' disguised as motivation. Many people are motivated but not self-directed, and you need both. A self-directed individual needs guidance but not much management. Be careful to see aspiration in a clear light.&lt;/p&gt;
&lt;p&gt;Lastly, do not dismiss but make room for intuition. This is one of those terms that are hard to define but not too difficult to recognize. Not too difficult because this trait is often expressed as a 'magical' ability to formulate solutions or ask the right questions. Ask hard, open-ended questions and observe what ensues. The answer doesn't have to be right, just penetrating or meandering towards plausible reasoning. Questions like those on an academic test (or puzzles if just a matter of memorization) that elicit some expected answers seldom highlight intuition.&lt;/p&gt;
&lt;hr&gt;&lt;/div&gt;</description><guid>https://epsilonheap.github.io/posts/wellspring-of-innovation/</guid><pubDate>Fri, 15 Feb 2019 15:08:00 GMT</pubDate></item><item><title>Ensemble Trees</title><link>https://epsilonheap.github.io/posts/ensemble-trees/</link><dc:creator>Your Name</dc:creator><description>&lt;div&gt;&lt;h2&gt;Chasing the Deep End&lt;/h2&gt;
&lt;p&gt;Deep this, deep that; not that there is anything wrong with that. Why? because deep neural-networks (now being referred to as &lt;a href="https://www.facebook.com/yann.lecun/posts/10155003011462143"&gt;differential programming&lt;/a&gt; by one of its pioneers) is one of milestone advancements in ML/AI and have many applications. So generally powerful and useful that it is now a fundamental design component for higher-level systems tailored for specific domains (see &lt;a href="https://www.youtube.com/watch?v=QuELiw8tbx8"&gt;Stanford lecture&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=oGk1v1jQITw"&gt;Deep Learning School presentation&lt;/a&gt; by &lt;a href="https://www.socher.org/"&gt;Richard Sochar&lt;/a&gt;). As an electrical engineer by academic title, this systems approach is comfortably familiar, very much like circuits design.&lt;/p&gt;
&lt;p&gt;Another 'component' that has been very useful, powerful, and efficient falls under the category of ensemble trees - especially the &lt;a href="https://en.wikipedia.org/wiki/Gradient_boosting"&gt;boosting&lt;/a&gt; variant. In a sense, ensemble trees are both deep and wide. The number of tree layers control depth versus breath, and an ensemble overlay offer wide exploration of features and relationships to observed outcomes. Yet more 'deepness' could be had by employing a component approach where elements are wired together.&lt;/p&gt;
&lt;p&gt;Table 10.1 of &lt;a href="https://web.stanford.edu/~hastie/ElemStatLearn/"&gt;'The Elements of Statistical Learning'&lt;/a&gt; compares trees-based methods to some contemporaries. Although much progress has been been made since its publication, most of the characterizations still hold. In particular, efficiency, forbearance with regards to feature data, and interpretability are outstanding advantages. &lt;/p&gt;
&lt;h2&gt;Results and Insights&lt;/h2&gt;
&lt;p&gt;These qualities are differentiators where decisions are costly in terms of the cost of doing experiments, collecting data, and unsatisfactory consequences. Of course, mistakes are unavoidable and tree-base methods are typically less accurate, but being clueless about why something works or fails can be a &lt;a href="https://www.risk.net/asset-management/6119616/blackrock-shelves-unexplainable-ai-liquidity-models"&gt;no go&lt;/a&gt; - you cannot learn from your mistakes. For medical use, a lack of reasoning is not comforting. In a research setting, gaining insights from interpretable models are more useful than raw accuracy. Tree-based methods can also be applied early on as part of a systematic application of data science - they are good at taking available data without much preprocessing and identifying influential features. They are fast and relatively easy to use.&lt;/p&gt;
&lt;h2&gt;Building on a Good Idea&lt;/h2&gt;
&lt;p&gt;Ensembles, tree-based models, bagging, and boosting make for great companions. Some nifty variants on this combinations are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Random Forests&lt;/li&gt;
&lt;li&gt;Xgboost&lt;/li&gt;
&lt;li&gt;Catboost&lt;/li&gt;
&lt;li&gt;mboost&lt;/li&gt;
&lt;li&gt;gamboostLSS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;[annotate later]&lt;/p&gt;
&lt;hr&gt;&lt;/div&gt;</description><guid>https://epsilonheap.github.io/posts/ensemble-trees/</guid><pubDate>Thu, 14 Feb 2019 16:35:13 GMT</pubDate></item><item><title>Sparse High-Dimensional Regression</title><link>https://epsilonheap.github.io/posts/sparse-high-dimensional-regression/</link><dc:creator>Your Name</dc:creator><description>&lt;div&gt;&lt;h2&gt;What!?&lt;/h2&gt;
&lt;p&gt;This one falls under the 'mind-bending' category that has initiated a personal re-examination of the whole practice of regression. Bertsimas and Van Parys released &lt;a href="https://arxiv.org/abs/1709.10029"&gt;'Sparse High-Dimensional Regression: Exact Scalable Algorithms and Phase Transitions'&lt;/a&gt; upon the world and upended my beliefs. &lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.mit.edu/~dbertsim/"&gt;Professor Bertsimas's&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=7w9aRrYgGEs"&gt;Hotelling lecture&lt;/a&gt; is a good video presentation of his approach to the topic.
[Note: Optimization is a foundational showcase of the power of computing - pay attention to this field.] An earlier paper &lt;a href="https://arxiv.org/abs/1507.03133"&gt;'Best Subset Selection via a Modern Optimization Lens'&lt;/a&gt; started the buzz and elicited Trevor Hastie, Robert Tibshirani, and Ryan Tibshirani to respond with &lt;a href="https://arxiv.org/abs/1707.08692"&gt;'Extended Comparisons of Best Subset Selection, Forward Stepwise Selection, and the Lasso'&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The problem: regression, although very useful, has always had some associated arbitrariness. Leo Brieman's dissatisfaction with the somewhat 'fickle' nature of statistical regression lead him to develop predictive methods based on algorithms rather than statistical models. One of Brieman's &lt;a href="https://www.stat.berkeley.edu/~breiman/wald2002-2.pdf"&gt;Wald Lecture&lt;/a&gt; describes the state of affairs that motivated his work. For instance, given the same data, practitioners would arrive at a multitude of different models with different predictor variables that should have similar predictive abilities. The problem was not with the skills of those doing the analysis (page 11 of lecture) but with the method itself. Bertsimas and Van Parys suggest that a more 'exact' approach could be at hand.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Mixed Integer Optimization (MIO)&lt;/h2&gt;
&lt;p&gt;Loosely stated, MIO has been shown in practice to be highly efficient and effective in regularizing to the zero norm of a regression model; finding the support of the variables under selection. In theory, this should be a daunting prospect (NP-Complete) and rather hopeless for very high dimensional problems. This paper shows otherwise and hints at some sort of phase-transition taking place. Phase-transitions are typically associated with the universal behavior of matter in different phases undergoing a competition to reach stability near what are known as 'critical' points where both phases co-exist - again loosely stated.&lt;/p&gt;
&lt;p&gt;MIO are mysterious to me and begs for study. What could constitute the two-phases (duality is two faces of the same problem)? What exactly are the mechanisms of MIO?&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;hr&gt;&lt;/div&gt;</description><guid>https://epsilonheap.github.io/posts/sparse-high-dimensional-regression/</guid><pubDate>Mon, 11 Feb 2019 12:29:47 GMT</pubDate></item><item><title>Kelly with Drawdown</title><link>https://epsilonheap.github.io/posts/kelly-with-drawdown/</link><dc:creator>Your Name</dc:creator><description>&lt;div&gt;&lt;h2&gt;Kelly Gambling&lt;/h2&gt;
&lt;p&gt;Portfolio management is a pillar of asset management. Given resources and many avenues for allocating said resources, how to allocate so as to achieve a desired effect? With regards to money, more is considered best, and with little chance of losing much would be ideal. Two views have received considerable attention on this topic. One approach is known as '&lt;a href="https://www.investopedia.com/terms/m/modernportfoliotheory.asp"&gt;Modern Portfolio Theory&lt;/a&gt;' and the other is '&lt;a href="https://www.investopedia.com/articles/trading/04/091504.asp"&gt;Kelly Criterion&lt;/a&gt;'. Warren Buffet &lt;a href="http://undergroundvalue.blogspot.com/2008/02/notes-from-buffett-meeting-2152008_23.html"&gt;explains&lt;/a&gt; the difference when asked by students: take advantage of an 'edge' where possible, and seek safety in numbers otherwise.&lt;/p&gt;
&lt;p&gt;Modern Portfolio Theory is taught in academia, and Kelly Criterion is mostly a method encountered by those asking 'is there another way?' This article on &lt;a href="https://ergodicityeconomics.com/2018/02/16/the-trouble-with-bernoulli-1738/#more-3206"&gt;Bernoulli&lt;/a&gt; and another on &lt;a href="https://ergodicityeconomics.com/2017/07/18/doing-a-laplace/#more-102"&gt;Laplace&lt;/a&gt; examine the fundamental difference of the two views and advocates for the Kelly approach. The latter article explains why the Kelly approach is less well known. Incidently, the criterion named after John Kelly is an of application of Claude Shannon's work to gambling - as detailed by &lt;a href="http://home.williampoundstone.net/Kelly.htm"&gt;William Poundstone&lt;/a&gt;. Yet another area Leo Breiman focused his energy was with &lt;a href="https://projecteuclid.org/euclid.bsmsp/1200512159"&gt;'Optimal Gambling Systems for Favorable Games'&lt;/a&gt; where he explored the idea introduced by Kelly. &lt;a href="http://www-isl.stanford.edu/~cover/portfolio-theory.html"&gt;Thomas Cover&lt;/a&gt; of information theory fame also examined the subject matter as well.&lt;/p&gt;
&lt;p&gt;Enzo Busseti, Ernest Ryu, and &lt;a href="http://web.stanford.edu/~boyd/"&gt;Stephen Boyd&lt;/a&gt; (who oversees several areas of exciting research) published '&lt;a href="https://arxiv.org/abs/1603.06183"&gt;Risk-Constrained Kelly Gambling&lt;/a&gt;' which derives an approach to incorporate &lt;a href="https://www.investopedia.com/terms/d/drawdown.asp"&gt;drawdown&lt;/a&gt; with Kelly gambling. When speaking of risks, drawdown would be top of the list for many - more so when following a Kelly approach. With concentrated positions, the prospects of some big bets going awry would be disastrous whereas a highly diversified portfolio would suffer the same fate as the market as a whole. This paper combines Kelly gambling with risk aversion in a systematic way, and do so by casting the problem as &lt;a href="http://web.stanford.edu/~boyd/cvxbook/"&gt;convex optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{cl}
maximize &amp;amp; \mathbb{E}\log({r^\intercal b}) \cr[2ex]
subject \; to &amp;amp; \begin{aligned}
\mathbb{E}(r^\intercal b)^{-\lambda} &amp;amp; \leq 1 \cr
\mathbb{I}^\intercal b &amp;amp; = 1, b \geq 0
\end{aligned}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;where \(r\) is a matrix of returns for a set of investments, \(b\) is a vector of allocations, and \(r^\intercal b\) is then the gain in wealth of a portfolio. \( \mathbb{E}(r^\intercal b)^{-\lambda} \leq 1 \) will be expanded upon later. This constraint effectively bounds how much risk one is willing to take.&lt;/p&gt;
&lt;p&gt;A key result of the paper is that \( \mathbb{E}(r^\intercal b)^{-\lambda} \leq 1 \Rightarrow \mathbb{Prob}(W^{min} \lt \alpha) \lt \beta\). That is, the probability \(W^{min}\), a random variable of the minimum wealth, of being less that \(\alpha\) is less than \(\beta\), and is equivalent to the aforementioned 'degree' of risk constraint. A different symbolic way to see the interplay of \(\alpha, \: \beta, \: and, \: \lambda \) is \( \mathbb{Prob}(W^{min} \lt \alpha) \lt \alpha^\lambda = \beta \) where \( \lambda = \frac{\log\beta}{\log \alpha} \).&lt;/p&gt;
&lt;p&gt;The paper carefully presents several variations of the Kelly gamble, establishes bounds, and performs a comparative study, but what is highlighted above constitutes the heart of the matter for realistic applications.&lt;/p&gt;
&lt;hr&gt;&lt;/div&gt;</description><guid>https://epsilonheap.github.io/posts/kelly-with-drawdown/</guid><pubDate>Thu, 07 Feb 2019 20:43:54 GMT</pubDate></item><item><title>Time series</title><link>https://epsilonheap.github.io/posts/Time%20series/</link><dc:creator>Your Name</dc:creator><description>&lt;div&gt;&lt;h2&gt;Time Series&lt;/h2&gt;
&lt;p&gt;Educated as an Electrical engineer early in life and having more recently worked as a 'quant' for many years, the topic of time series analysis has evolved from a clean one to a bewildering hodgepodge of complications; pretty much all of it because of the stochastic nature of markets.&lt;/p&gt;
&lt;p&gt;Some fundamental issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Transactional - data from markets are records of transactional events and not uniformly sampled; therefore, times series are non-homogeneous.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stationarity - market data are not necessarily stationary and often have complicated time-delayed feedback structures. Moreover, change points (or regime shifts) are not uncommon.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ergodicity - practitioners often analyze time series to aid in prediction; but under what condition can the histories of (single realizations of) sampled paths be considered representative of future behavior?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are other issues like low signal-to-noise, high dimensionality, missing values, and outliers. Although these are important, the three highlighted undermine the applicability of traditional methods.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Transactional&lt;/h3&gt;
&lt;p&gt;Traditionally an engineer dealing with signals mainly work with Fourier transforms and related objects.&lt;/p&gt;
&lt;p&gt;$$
f(x) = \int_{-\infty}^{\infty} \hat f(\xi) e^{2 \pi i \xi x} d\xi 
$$&lt;/p&gt;
&lt;p&gt;Clean and elegant. This formalism is fundamental and will not likely ever diminish in stature. Yet quantitative-minded participants in finance, in particular currency traders of the late 1990's, pointed out a significant characteristics of the data of their trade that needed addressing:
&lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=208278"&gt;Operators on Inhomogeneous Time Series&lt;/a&gt;. A central idea underlying inhomogeneous time series is that 'time' is proportional to the density of 'activity'. With regards to the 'information' content of inhomogeneous market data, this paper &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1989555"&gt;'Discerning Information from Trade Data'&lt;/a&gt; describes a aggregated 'tick' or 'bar' approach - essentially integrating pieces of data partitioned by time into informational units for analysis. Aside: a good presentation of the many pitfalls encountered by financial professionals are in Marcos Lopez de Prado's &lt;a href="https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482109"&gt;'Advances in Financial Machine Learning'&lt;/a&gt;. A Jupyter notebook in the blog post &lt;a href="http://www.blackarbs.com/blog/exploring-alternative-price-bars"&gt;'Exploring Alternative Price Bars'&lt;/a&gt; illustrates this approach.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Non-Stationary Data&lt;/h3&gt;
&lt;p&gt;Over an intermediate time-span, market data fluctuate in a relatively well-behaved manner, and statistical observations like mean-reversion are relevant and expected. On a longer time-scale, statistically 'rare' events are not so uncommon in real markets. In other words, statistical models stop making sense when markets go wild - a good indicator that market prices are not necessarily stationary (see &lt;a href="https://www.youtube.com/watch?v=Pn_RiDbK82M&amp;amp;t=160s"&gt;Integration, Cointegration, and Stationarity&lt;/a&gt; for examples of non-stationary time series that are rather tame. Beware though that using integer differentiation may remove information content. See chapter 5 of &lt;a href="https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482109"&gt;'Advances in Financial Machine Learning'&lt;/a&gt; or look up fractional differentiation for financial data.)&lt;/p&gt;
&lt;!---
memory effects
--&gt;

&lt;p&gt;Bottom line is that market data needs to be analyzed and transformed before statistical methods are applicable as intended. Then again, many methods of Machine Learning work well in practice without a rigorous understanding of why they work. In any case, a usual suspect for why a model has limited or no predictive ability post construction is because the model was formulated with non-stationary data. Good sites examining fundamental issues in applying scientific theory to economics is &lt;a href="https://ergodicityeconomics.com/"&gt;Ergodicity Economics&lt;/a&gt; and to markets is &lt;a href="http://www.quantresearch.info/"&gt;Quantitative Finance&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Ergodicity&lt;/h3&gt;
&lt;p&gt;Equivalence of ensemble-average and time-average with respect to some sampled statistics is an important subject. &lt;a href="https://arxiv.org/abs/1401.7224"&gt;Gaveau and Schulman&lt;/a&gt; questioned whether ergodicity is a reasonable hypothesis - many applications only require 'reasonably'-sized samples. An article by &lt;a href="https://statweb.stanford.edu/~cgates/PERSI/papers/mixing.pdf"&gt;Persi Diaconis&lt;/a&gt; explore this for practitioners. &lt;a href="https://www.youtube.com/watch?v=LGqOH3sYmQA"&gt;Ole Peters&lt;/a&gt; has put forth commendable effort at explaining
the real-world implications of using ensemble quantities when individuals experience but one life. &lt;a href="https://www.youtube.com/watch?v=qA_6BWkC4og"&gt;Nassim Taleb&lt;/a&gt; discusses ergodicity from a trader's perspective.&lt;/p&gt;
&lt;p&gt;Even though the age of immense computing power and massive data collection is upon us, A better understanding of how many representative examples are enough for a good approximation is more important then ever. Reason being that society is putting more faith than ever into systems trained by examples, whether they are generated (games and such) or collected.&lt;/p&gt;
&lt;hr&gt;&lt;/div&gt;</description><guid>https://epsilonheap.github.io/posts/Time%20series/</guid><pubDate>Mon, 04 Feb 2019 19:34:54 GMT</pubDate></item><item><title>first tiny bit</title><link>https://epsilonheap.github.io/posts/first-tiny-bit/</link><dc:creator>Your Name</dc:creator><description>&lt;p&gt;Trying out different site generators. Nikola is first. Yea! \( e^{ix} = \cos{x} + i \sin{x} \) works fine!
Now try adding [GatsbyJS](&lt;a class="reference external" href="https://www.gatsbyjs.org/"&gt;https://www.gatsbyjs.org/&lt;/a&gt;) under the Nikola structure to take advantage of the dynamic features of that infrastructure.&lt;/p&gt;</description><guid>https://epsilonheap.github.io/posts/first-tiny-bit/</guid><pubDate>Sat, 02 Feb 2019 15:07:54 GMT</pubDate></item></channel></rss>